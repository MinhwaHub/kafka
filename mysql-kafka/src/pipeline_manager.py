#!/usr/bin/env python3
"""
MySQL â†’ Kafka â†’ S3 íŒŒì´í”„ë¼ì¸ ê´€ë¦¬ ìŠ¤í¬ë¦½íŠ¸
ì‹¤í–‰ ì¤‘ì¸ íŒŒì´í”„ë¼ì¸ì„ ì™¸ë¶€ì—ì„œ ì œì–´í•  ìˆ˜ ìˆëŠ” ë…ë¦½ì ì¸ ê´€ë¦¬ ë„êµ¬
"""

import argparse
import json
import time
import signal
import os
import sys
from datetime import datetime
import psutil
import threading
from typing import Dict, List

# src ë””ë ‰í† ë¦¬ì—ì„œ ì‹¤í–‰í•˜ëŠ” ê²½ìš° ìƒìœ„ ë””ë ‰í† ë¦¬ë¥¼ Python pathì— ì¶”ê°€
if os.path.basename(os.getcwd()) == "src":
    parent_dir = os.path.dirname(os.getcwd())
    if parent_dir not in sys.path:
        sys.path.insert(0, parent_dir)

# config íŒŒì¼ì—ì„œ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
try:
    # src ë””ë ‰í† ë¦¬ì—ì„œ ì‹¤í–‰í•˜ëŠ” ê²½ìš°
    from config import (
        PIPELINES,
        PIPELINE_MANAGER_LOG,
        MAIN_PIPELINE_PID_FILE,
        PIPELINE_STDOUT_LOG,
        PIPELINE_STDERR_LOG,
        MAIN_SCRIPT_PATH,
        PIPELINE_STATUS_FILE,
        PIPELINE_COMMAND_FILE,
        SLACK_TOKEN,
        SLACK_CHANNEL_ID,
    )
except ImportError:
    try:
        # mysql-kafka ë£¨íŠ¸ì—ì„œ ì‹¤í–‰í•˜ëŠ” ê²½ìš°
        from src.config import (
            PIPELINES,
            PIPELINE_MANAGER_LOG,
            MAIN_PIPELINE_PID_FILE,
            PIPELINE_STDOUT_LOG,
            PIPELINE_STDERR_LOG,
            MAIN_SCRIPT_PATH,
            PIPELINE_STATUS_FILE,
            PIPELINE_COMMAND_FILE,
            SLACK_TOKEN,
            SLACK_CHANNEL_ID,
        )
    except ImportError:
        print(
            "âŒ Error: config.py file not found. Please ensure config.py exists in the src directory."
        )
        sys.exit(1)

# pipeline importë„ ì ì‘ì ìœ¼ë¡œ ì²˜ë¦¬
try:
    # src ë””ë ‰í† ë¦¬ì—ì„œ ì‹¤í–‰í•˜ëŠ” ê²½ìš°
    from pipeline import MySQLKafkaS3Pipeline
except ImportError:
    try:
        # mysql-kafka ë£¨íŠ¸ì—ì„œ ì‹¤í–‰í•˜ëŠ” ê²½ìš°
        from src.pipeline import MySQLKafkaS3Pipeline
    except ImportError:
        print("âŒ Error: pipeline.py file not found.")
        sys.exit(1)

from utils.circuit_breaker import CircuitBreakerState
from utils.slack_util import SlackMessenger


class PipelineManagerCLI:
    """
    íŒŒì´í”„ë¼ì¸ ê´€ë¦¬ CLI ë„êµ¬
    cmdì— ë”°ë¥¸ ì ˆì°¨ ë° ë¡œê·¸ ì¶œë ¥
    """

    def __init__(self):
        self.log_file = PIPELINE_MANAGER_LOG
        self.pid_file = MAIN_PIPELINE_PID_FILE  # ì‹¤í–‰ì¤‘ì¸ íŒŒì´í”„ë¼ì¸ í”„ë¡œì„¸ìŠ¤ ID

        # Slack messenger ì´ˆê¸°í™” (App ê¸°ë°˜)
        self.slack_messenger = SlackMessenger(
            token=SLACK_TOKEN,
            channel_id=SLACK_CHANNEL_ID,
        )

    def log_message(self, message: str):
        """ë¡œê·¸ ë©”ì‹œì§€ ì¶œë ¥ ë° íŒŒì¼ ì €ì¥"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_entry = f"[{timestamp}] {message}"
        print(log_entry)

        try:
            with open(self.log_file, "a", encoding="utf-8") as f:
                f.write(log_entry + "\n")
        except Exception as e:
            print(f"âš ï¸ Failed to write to log file: {e}")

    def is_pipeline_running(self):
        """íŒŒì´í”„ë¼ì¸ í”„ë¡œì„¸ìŠ¤ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸"""
        if not os.path.exists(self.pid_file):
            return False, None

        try:
            with open(self.pid_file, "r") as f:
                pid = int(f.read().strip())

            # PIDê°€ ì‹¤ì œë¡œ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸
            if psutil.pid_exists(pid):
                process = psutil.Process(pid)

                # í”„ë¡œì„¸ìŠ¤ ëª…ë ¹ì¤„ ì¸ìë¥¼ í™•ì¸í•´ì„œ main.pyê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ ì²´í¬
                try:
                    cmdline = process.cmdline()
                    # cmdlineì€ ['python', 'main.py'] í˜•íƒœ
                    if len(cmdline) >= 2 and "main.py" in cmdline[1]:
                        return True, pid
                except (psutil.AccessDenied, psutil.NoSuchProcess):
                    pass

            # PID íŒŒì¼ì´ ìˆì§€ë§Œ ì˜¬ë°”ë¥¸ í”„ë¡œì„¸ìŠ¤ê°€ ì•„ë‹ˆë©´ íŒŒì¼ ì‚­ì œ
            os.remove(self.pid_file)
            return False, None

        except Exception as e:
            self.log_message(f"âš ï¸ Error checking pipeline status: {e}")
            return False, None

    def start_pipeline(self):
        """íŒŒì´í”„ë¼ì¸ ì‹œì‘"""
        is_running, pid = self.is_pipeline_running()

        if is_running:
            self.log_message(f"âš ï¸ Pipeline is already running (PID: {pid})")
            return False

        self.log_message("ğŸš€ Starting pipeline...")

        # Slack ì•Œë¦¼: íŒŒì´í”„ë¼ì¸ ì‹œì‘ (ìƒíƒœ ì •ë³´ í¬í•¨)
        status_data = self.get_structured_status()
        status_blocks = self.format_status_for_slack(status_data)
        self.slack_messenger.send_slack(
            text="ğŸš€ *Pipeline is starting...*",
            # block_message=status_blocks,
            color="#36a64f",
        )

        try:
            # ë°±ê·¸ë¼ìš´ë“œì—ì„œ íŒŒì´í”„ë¼ì¸ ì‹œì‘
            import subprocess
            import sys

            # Python ê²½ë¡œì™€ main.py ê²½ë¡œ í™•ì¸
            python_path = sys.executable
            main_script = MAIN_SCRIPT_PATH

            if not os.path.exists(main_script):
                self.log_message(
                    f"âŒ Error: {main_script} not found in current directory"
                )
                self.send_slack_notification(
                    f"âŒ Pipeline start failed: {main_script} not found", "error"
                )
                return False

            # nohupìœ¼ë¡œ ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰
            with open(PIPELINE_STDOUT_LOG, "w") as stdout_file, open(
                PIPELINE_STDERR_LOG, "w"
            ) as stderr_file:
                process = subprocess.Popen(
                    [python_path, main_script],
                    stdout=stdout_file,
                    stderr=stderr_file,
                    start_new_session=True,
                )

                # PID ì €ì¥
                with open(self.pid_file, "w") as f:
                    f.write(str(process.pid))

                self.log_message(
                    f"âœ… Pipeline started successfully (PID: {process.pid})"
                )
                self.log_message(
                    f"ğŸ“‹ Logs: {PIPELINE_STDOUT_LOG} (stdout), {PIPELINE_STDERR_LOG} (stderr)"
                )

                # Slack ì•Œë¦¼: íŒŒì´í”„ë¼ì¸ ì‹œì‘ ì„±ê³µ (ì—…ë°ì´íŠ¸ëœ ìƒíƒœ ì •ë³´ í¬í•¨)
                time.sleep(1)  # í”„ë¡œì„¸ìŠ¤ê°€ ì™„ì „íˆ ì‹œì‘ë  ë•Œê¹Œì§€ ì ì‹œ ëŒ€ê¸°
                updated_status_data = self.get_structured_status()
                updated_status_blocks = self.format_status_for_slack(
                    updated_status_data
                )
                self.slack_messenger.send_slack(
                    text=f"âœ… *Pipeline started successfully (PID: {process.pid})*",
                    block_message=updated_status_blocks,
                    color="#36a64f",
                )

                return True

        except Exception as e:
            self.log_message(f"âŒ Failed to start pipeline: {e}")
            self.slack_messenger.send_slack(
                text=f"âŒ *Failed to start pipeline: {e}*", color="#ff0000"
            )
            return False

    def stop_pipeline(self):
        """íŒŒì´í”„ë¼ì¸ ì¤‘ì§€"""
        is_running, pid = self.is_pipeline_running()

        if not is_running:
            self.log_message("âš ï¸ Pipeline is not running")
            return False

        try:
            self.log_message(f"ğŸ›‘ Stopping pipeline (PID: {pid})...")

            # Slack ì•Œë¦¼: íŒŒì´í”„ë¼ì¸ ì¤‘ì§€ ì‹œì‘ (í˜„ì¬ ìƒíƒœ ì •ë³´ í¬í•¨)
            status_data = self.get_structured_status()
            status_blocks = self.format_status_for_slack(status_data)
            self.slack_messenger.send_slack(
                text=f"ğŸ›‘ *Stopping pipeline (PID: {pid})...*",
                # block_message=status_blocks,
                color="#ffa500",
            )

            process = psutil.Process(pid)

            # SIGTERMìœ¼ë¡œ ìš°ì•„í•œ ì¢…ë£Œ ì‹œë„
            process.terminate()

            # ìµœëŒ€ 10ì´ˆ ëŒ€ê¸°
            try:
                process.wait(timeout=10)
                self.log_message("âœ… Pipeline stopped gracefully")
                # Slack ì•Œë¦¼: íŒŒì´í”„ë¼ì¸ ì •ìƒ ì¤‘ì§€ (ì—…ë°ì´íŠ¸ëœ ìƒíƒœ ì •ë³´ í¬í•¨)
                updated_status_data = self.get_structured_status()
                updated_status_blocks = self.format_status_for_slack(
                    updated_status_data
                )
                self.slack_messenger.send_slack(
                    text="âœ… *Pipeline stopped gracefully*",
                    block_message=updated_status_blocks,
                    color="#36a64f",
                )
            except psutil.TimeoutExpired:
                # ê°•ì œ ì¢…ë£Œ
                self.log_message("âš ï¸ Graceful shutdown timeout. Force killing...")
                process.kill()
                process.wait()
                self.log_message("âœ… Pipeline force stopped")
                # Slack ì•Œë¦¼: íŒŒì´í”„ë¼ì¸ ê°•ì œ ì¤‘ì§€ (ì—…ë°ì´íŠ¸ëœ ìƒíƒœ ì •ë³´ í¬í•¨)
                updated_status_data = self.get_structured_status()
                updated_status_blocks = self.format_status_for_slack(
                    updated_status_data
                )
                self.slack_messenger.send_slack(
                    text="âš ï¸ *Pipeline force stopped (timeout)*",
                    block_message=updated_status_blocks,
                    color="#ffa500",
                )

            # PID íŒŒì¼ ì‚­ì œ
            if os.path.exists(self.pid_file):
                os.remove(self.pid_file)

            # ëŸ°íƒ€ì„ ìƒíƒœ íŒŒì¼ ì—…ë°ì´íŠ¸ (ëª¨ë“  íŒŒì´í”„ë¼ì¸ì„ ì¤‘ì§€ ìƒíƒœë¡œ ì„¤ì •)
            self.update_status_on_shutdown()

            return True

        except Exception as e:
            self.log_message(f"âŒ Failed to stop pipeline: {e}")
            self.slack_messenger.send_slack(
                text=f"âŒ *Failed to stop pipeline: {e}*", color="#ff0000"
            )
            return False

    def update_status_on_shutdown(self):
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì¢…ë£Œì‹œ ìƒíƒœ íŒŒì¼ ì—…ë°ì´íŠ¸"""
        try:
            status_file = PIPELINE_STATUS_FILE

            # ê¸°ì¡´ ìƒíƒœ íŒŒì¼ì´ ìˆìœ¼ë©´ ë¡œë“œ, ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
            status_data = {}
            if os.path.exists(status_file):
                try:
                    with open(status_file, "r") as f:
                        status_data = json.load(f)
                except:
                    pass

            # ëª¨ë“  ì„¤ì •ëœ íŒŒì´í”„ë¼ì¸ì„ ì¤‘ì§€ ìƒíƒœë¡œ ì—…ë°ì´íŠ¸
            for pipeline in PIPELINES:
                pipeline_name = pipeline["name"]

                # ê¸°ì¡´ ë°ì´í„° ë³´ì¡´ (ì¬ì‹œì‘ íšŸìˆ˜, last_processed_id ë“±)
                if pipeline_name in status_data:
                    restart_count = status_data[pipeline_name].get("restart_count", 0)
                    last_processed_id = status_data[pipeline_name].get(
                        "last_processed_id", 0
                    )
                    # ê¸°ì¡´ is_failed ìƒíƒœë„ ë³´ì¡´ (ì •ìƒ ì¢…ë£Œë¼ë©´ false ìœ ì§€)
                    current_is_failed = status_data[pipeline_name].get(
                        "is_failed", False
                    )
                else:
                    restart_count = 0
                    last_processed_id = 0
                    current_is_failed = False

                status_data[pipeline_name] = {
                    "enabled": False,  # ì¢…ë£Œë˜ì—ˆìœ¼ë¯€ë¡œ ë¹„í™œì„±í™”
                    "is_failed": current_is_failed,  # ê¸°ì¡´ ìƒíƒœ ë³´ì¡´
                    "restart_count": restart_count,
                    "last_processed_id": last_processed_id,  # ë³´ì¡´
                    "table_name": pipeline["mysql"]["table"],
                    "kafka_topic": pipeline["kafka"]["topic"],
                }

            # ì—…ë°ì´íŠ¸ëœ ìƒíƒœë¥¼ íŒŒì¼ì— ì €ì¥
            with open(status_file, "w") as f:
                json.dump(status_data, f, indent=2)

            self.log_message("ğŸ“ Pipeline status file updated on shutdown")

        except Exception as e:
            self.log_message(f"âš ï¸ Failed to update status file on shutdown: {e}")

    def update_pipeline_success_status(
        self, pipeline_name: str, last_processed_id: int = None
    ):
        """íŒŒì´í”„ë¼ì¸ ì„±ê³µ ìƒíƒœ ì—…ë°ì´íŠ¸ (ì‹¤ì‹œê°„)"""
        try:
            status_file = PIPELINE_STATUS_FILE

            # ê¸°ì¡´ ìƒíƒœ íŒŒì¼ ë¡œë“œ
            status_data = {}
            if os.path.exists(status_file):
                try:
                    with open(status_file, "r") as f:
                        status_data = json.load(f)
                except:
                    pass

            # í•´ë‹¹ íŒŒì´í”„ë¼ì¸ ì°¾ê¸°
            pipeline_config = None
            for pipeline in PIPELINES:
                if pipeline["name"] == pipeline_name:
                    pipeline_config = pipeline
                    break

            if not pipeline_config:
                return

            # ì„±ê³µ ìƒíƒœë¡œ ì—…ë°ì´íŠ¸
            if pipeline_name not in status_data:
                status_data[pipeline_name] = {}

            current_data = status_data[pipeline_name]

            status_data[pipeline_name] = {
                "enabled": True,  # ì‹¤í–‰ ì¤‘
                "is_failed": False,  # ì„±ê³µ ìƒíƒœ
                "restart_count": current_data.get("restart_count", 0),
                "last_processed_id": (
                    last_processed_id
                    if last_processed_id is not None
                    else current_data.get("last_processed_id", 0)
                ),
                "table_name": pipeline_config["mysql"]["table"],
                "kafka_topic": pipeline_config["kafka"]["topic"],
            }

            # íŒŒì¼ ì €ì¥
            with open(status_file, "w") as f:
                json.dump(status_data, f, indent=2)

        except Exception as e:
            self.log_message(f"âš ï¸ Failed to update pipeline success status: {e}")

    def restart_pipeline(self):
        """íŒŒì´í”„ë¼ì¸ ì¬ì‹œì‘"""
        self.log_message("ğŸ”„ Restarting pipeline...")

        # ë¨¼ì € ì¤‘ì§€
        self.stop_pipeline()

        # ì ì‹œ ëŒ€ê¸°
        time.sleep(3)

        # ë‹¤ì‹œ ì‹œì‘
        return self.start_pipeline()

    def status_pipeline(self):
        """íŒŒì´í”„ë¼ì¸ ìƒíƒœ í™•ì¸"""
        is_running, pid = self.is_pipeline_running()

        self.log_message("ğŸ“Š Pipeline Status:")

        if is_running:
            try:
                process = psutil.Process(pid)
                create_time = datetime.fromtimestamp(process.create_time())
                memory_info = process.memory_info()
                cpu_percent = process.cpu_percent()

                self.log_message(f"  Status: âœ… RUNNING")
                self.log_message(f"  PID: {pid}")
                self.log_message(
                    f"  Started: {create_time.strftime('%Y-%m-%d %H:%M:%S')}"
                )
                self.log_message(f"  Memory: {memory_info.rss / 1024 / 1024:.1f} MB")
                self.log_message(f"  CPU: {cpu_percent}%")

                # ìì‹ í”„ë¡œì„¸ìŠ¤ (ìŠ¤ë ˆë“œ) ê°œìˆ˜
                try:
                    children = process.children()
                    self.log_message(f"  Child Processes: {len(children)}")
                except:
                    pass

            except Exception as e:
                self.log_message(f"  Status: âŒ ERROR - {e}")
        else:
            self.log_message(f"  Status: âŒ NOT RUNNING")

        # ëŸ°íƒ€ì„ ìƒíƒœ íŒŒì¼ ë¡œë“œ
        runtime_status = {}
        status_file = PIPELINE_STATUS_FILE
        try:
            if os.path.exists(status_file):
                with open(status_file, "r") as f:
                    runtime_status = json.load(f)
        except Exception as e:
            self.log_message(f"âš ï¸ Warning: Could not load runtime status: {e}")

        # ì„¤ì •ëœ íŒŒì´í”„ë¼ì¸ ì •ë³´
        self.log_message(f"\nğŸ“‹ Configured Pipelines ({len(PIPELINES)}):")
        for i, pipeline in enumerate(PIPELINES, 1):
            pipeline_name = pipeline["name"]

            # ëŸ°íƒ€ì„ ìƒíƒœê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ config ê¸°ë³¸ê°’ ì‚¬ìš©
            if pipeline_name in runtime_status:
                runtime_info = runtime_status[pipeline_name]
                is_enabled = runtime_info.get("enabled", True)
                is_failed = runtime_info.get("is_failed", False)
                restart_count = runtime_info.get("restart_count", 0)

                # ìƒíƒœ ì•„ì´ì½˜ ê²°ì •
                if is_failed:
                    status_icon = "âŒ"
                    status_text = " (FAILED)"
                elif not is_enabled:
                    status_icon = "â¸ï¸"
                    status_text = " (STOPPED)"
                else:
                    status_icon = "âœ…"
                    status_text = ""

                # ì¬ì‹œì‘ íšŸìˆ˜ í‘œì‹œ
                restart_info = (
                    f" (restarts: {restart_count})" if restart_count > 0 else ""
                )

            else:
                # ëŸ°íƒ€ì„ ìƒíƒœê°€ ì—†ìœ¼ë©´ config ê¸°ë³¸ê°’ ì‚¬ìš©
                is_enabled = pipeline.get("enabled", True)
                status_icon = "âœ…" if is_enabled else "âŒ"
                status_text = " (config default)"
                restart_info = ""

            self.log_message(
                f"  {i}. {status_icon} {pipeline_name}{status_text}{restart_info}"
            )
            self.log_message(f"     Table: {pipeline['mysql']['table']}")
            self.log_message(f"     Topic: {pipeline['kafka']['topic']}")

        # ë¡œê·¸ íŒŒì¼ ì •ë³´
        self.log_message(f"\nğŸ“„ Log Files:")
        for log_file in [PIPELINE_STDOUT_LOG, PIPELINE_STDERR_LOG, self.log_file]:
            if os.path.exists(log_file):
                stat = os.stat(log_file)
                size = stat.st_size / 1024  # KB
                modified = datetime.fromtimestamp(stat.st_mtime)
                self.log_message(
                    f"  {log_file}: {size:.1f} KB (modified: {modified.strftime('%Y-%m-%d %H:%M:%S')})"
                )
            else:
                self.log_message(f"  {log_file}: Not found")

    def show_logs(self, lines: int = 50):
        """ìµœê·¼ ë¡œê·¸ ë³´ê¸°"""
        log_files = {
            "stdout": PIPELINE_STDOUT_LOG,
            "stderr": PIPELINE_STDERR_LOG,
            "manager": self.log_file,
        }

        for log_type, log_file in log_files.items():
            if os.path.exists(log_file):
                self.log_message(
                    f"\nğŸ“„ {log_type.upper()} - Last {lines} lines from {log_file}:"
                )
                try:
                    with open(log_file, "r", encoding="utf-8") as f:
                        all_lines = f.readlines()
                        recent_lines = (
                            all_lines[-lines:] if len(all_lines) > lines else all_lines
                        )

                        for line in recent_lines:
                            print(f"  {line.rstrip()}")

                except Exception as e:
                    self.log_message(f"âš ï¸ Error reading {log_file}: {e}")
            else:
                self.log_message(f"\nğŸ“„ {log_type.upper()}: {log_file} not found")

    def restart_single_pipeline(self, pipeline_name: str):
        """ê°œë³„ íŒŒì´í”„ë¼ì¸ ì¬ì‹œì‘"""
        is_running, pid = self.is_pipeline_running()

        if not is_running:
            self.log_message(
                "âŒ Pipeline is not running. Cannot restart individual pipeline."
            )
            return False

        # ê°œë³„ íŒŒì´í”„ë¼ì¸ ì¬ì‹œì‘ ëª…ë ¹ íŒŒì¼ ìƒì„±
        command_file = PIPELINE_COMMAND_FILE
        command = f"restart {pipeline_name}"

        try:
            with open(command_file, "w") as f:
                f.write(command)

            self.log_message(f"ğŸ”„ Restart command sent for pipeline '{pipeline_name}'")
            self.log_message(f"ğŸ“ Command file created: {command_file}")

            # ì ì‹œ ëŒ€ê¸° í›„ ëª…ë ¹ íŒŒì¼ì´ ì²˜ë¦¬ë˜ì—ˆëŠ”ì§€ í™•ì¸
            max_wait = 10  # 10ì´ˆ ëŒ€ê¸°
            for i in range(max_wait):
                time.sleep(1)
                if not os.path.exists(command_file):
                    self.log_message(f"âœ… Command processed successfully")
                    return True

            self.log_message(
                f"âš ï¸ Command file still exists after {max_wait}s. Check pipeline logs."
            )
            return False

        except Exception as e:
            self.log_message(f"âŒ Failed to send restart command: {e}")
            return False

    def stop_single_pipeline(self, pipeline_name: str):
        """ê°œë³„ íŒŒì´í”„ë¼ì¸ ì¤‘ì§€"""
        is_running, pid = self.is_pipeline_running()

        if not is_running:
            self.log_message(
                "âŒ Pipeline is not running. Cannot stop individual pipeline."
            )
            return False

        # ê°œë³„ íŒŒì´í”„ë¼ì¸ ì¤‘ì§€ ëª…ë ¹ íŒŒì¼ ìƒì„±
        command_file = PIPELINE_COMMAND_FILE
        command = f"stop {pipeline_name}"

        try:
            with open(command_file, "w") as f:
                f.write(command)

            self.log_message(f"ğŸ›‘ Stop command sent for pipeline '{pipeline_name}'")
            self.log_message(f"ğŸ“ Command file created: {command_file}")

            # ì ì‹œ ëŒ€ê¸° í›„ ëª…ë ¹ íŒŒì¼ì´ ì²˜ë¦¬ë˜ì—ˆëŠ”ì§€ í™•ì¸
            max_wait = 10  # 10ì´ˆ ëŒ€ê¸°
            for i in range(max_wait):
                time.sleep(1)
                if not os.path.exists(command_file):
                    self.log_message(f"âœ… Command processed successfully")
                    return True

            self.log_message(
                f"âš ï¸ Command file still exists after {max_wait}s. Check pipeline logs."
            )
            return False

        except Exception as e:
            self.log_message(f"âŒ Failed to send stop command: {e}")
            return False

    def send_slack_notification(self, message: str, message_type: str = "info"):
        """Slack ì•Œë¦¼ ì „ì†¡"""
        try:
            if message_type == "error":
                color = "#ff0000"  # ë¹¨ê°„ìƒ‰
                alert_message = f"ğŸš¨ *Pipeline Manager Alert*"
            elif message_type == "warning":
                color = "#ffa500"  # ì£¼í™©ìƒ‰
                alert_message = f"*Pipeline Manager Warning*"
            else:  # info
                color = "#36a64f"  # ì´ˆë¡ìƒ‰
                alert_message = f"*Pipeline Manager Info*"

            alert_message += f"\n{message}"

            self.slack_messenger.send_slack(text=alert_message, color=color)
            self.log_message(f"ğŸ“¨ Slack notification sent: {message}")
        except Exception as e:
            self.log_message(f"âš ï¸ Failed to send Slack notification: {e}")

    def get_structured_status(self) -> dict:
        """íŒŒì´í”„ë¼ì¸ ìƒíƒœë¥¼ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ ë°˜í™˜"""
        is_running, pid = self.is_pipeline_running()

        status_data = {
            "pipeline_running": is_running,
            "pid": pid,
            "pipelines": [],
            "log_files": [],
        }

        # í”„ë¡œì„¸ìŠ¤ ì •ë³´
        if is_running:
            try:
                process = psutil.Process(pid)
                create_time = datetime.fromtimestamp(process.create_time())
                memory_info = process.memory_info()
                cpu_percent = process.cpu_percent()

                status_data["process_info"] = {
                    "started": create_time.strftime("%Y-%m-%d %H:%M:%S"),
                    "memory_mb": round(memory_info.rss / 1024 / 1024, 1),
                    "cpu_percent": cpu_percent,
                }

                try:
                    children = process.children()
                    status_data["process_info"]["child_processes"] = len(children)
                except:
                    status_data["process_info"]["child_processes"] = 0

            except Exception:
                status_data["process_info"] = {"error": "Failed to get process info"}

        # ëŸ°íƒ€ì„ ìƒíƒœ íŒŒì¼ ë¡œë“œ
        runtime_status = {}
        try:
            if os.path.exists(PIPELINE_STATUS_FILE):
                with open(PIPELINE_STATUS_FILE, "r") as f:
                    runtime_status = json.load(f)
        except Exception:
            pass

        # íŒŒì´í”„ë¼ì¸ ì •ë³´
        for i, pipeline in enumerate(PIPELINES, 1):
            pipeline_name = pipeline["name"]
            pipeline_info = {
                "name": pipeline_name,
                "table": pipeline["mysql"]["table"],
                "topic": pipeline["kafka"]["topic"],
            }

            if pipeline_name in runtime_status:
                runtime_info = runtime_status[pipeline_name]
                is_enabled = runtime_info.get("enabled", True)
                is_failed = runtime_info.get("is_failed", False)
                restart_count = runtime_info.get("restart_count", 0)

                if is_failed:
                    pipeline_info["status"] = "FAILED"
                    pipeline_info["status_icon"] = "âŒ"
                elif not is_enabled:
                    pipeline_info["status"] = "STOPPED"
                    pipeline_info["status_icon"] = "â¸ï¸"
                else:
                    pipeline_info["status"] = "RUNNING"
                    pipeline_info["status_icon"] = "âœ…"

                pipeline_info["restart_count"] = restart_count
            else:
                is_enabled = pipeline.get("enabled", True)
                pipeline_info["status"] = "RUNNING" if is_enabled else "DISABLED"
                pipeline_info["status_icon"] = "âœ…" if is_enabled else "âŒ"
                pipeline_info["restart_count"] = 0

            status_data["pipelines"].append(pipeline_info)

        # ë¡œê·¸ íŒŒì¼ ì •ë³´
        for log_name, log_file in [
            ("stdout", PIPELINE_STDOUT_LOG),
            ("stderr", PIPELINE_STDERR_LOG),
            ("manager", self.log_file),
        ]:
            if os.path.exists(log_file):
                stat = os.stat(log_file)
                size_kb = round(stat.st_size / 1024, 1)
                modified = datetime.fromtimestamp(stat.st_mtime)
                status_data["log_files"].append(
                    {
                        "name": log_name,
                        "path": log_file,
                        "size_kb": size_kb,
                        "modified": modified.strftime("%Y-%m-%d %H:%M:%S"),
                    }
                )
            else:
                status_data["log_files"].append(
                    {"name": log_name, "path": log_file, "exists": False}
                )

        return status_data

    def format_status_for_slack(self, status_data: dict) -> list:
        """êµ¬ì¡°í™”ëœ ìƒíƒœ ë°ì´í„°ë¥¼ Slack block í˜•íƒœë¡œ ë³€í™˜"""
        blocks = []

        # ë©”ì¸ ìƒíƒœ
        main_status = "ğŸŸ¢ RUNNING" if status_data["pipeline_running"] else "ğŸ”´ STOPPED"
        status_text = f"*Pipeline Status:* {main_status}"

        if status_data["pipeline_running"] and "process_info" in status_data:
            proc_info = status_data["process_info"]
            if "error" not in proc_info:
                status_text += f"\nâ€¢ PID: {status_data['pid']}"
                status_text += f"\nâ€¢ Started: {proc_info['started']}"
                status_text += f"\nâ€¢ Memory: {proc_info['memory_mb']} MB"
                status_text += f"\nâ€¢ Child Processes: {proc_info['child_processes']}"

        blocks.append(
            {"type": "section", "text": {"type": "mrkdwn", "text": status_text}}
        )

        # íŒŒì´í”„ë¼ì¸ ëª©ë¡
        if status_data["pipelines"]:
            pipeline_text = (
                f"*Configured Pipelines ({len(status_data['pipelines'])}):*\n"
            )
            for i, pipeline in enumerate(status_data["pipelines"], 1):
                restart_info = (
                    f" (restarts: {pipeline['restart_count']})"
                    if pipeline["restart_count"] > 0
                    else ""
                )
                pipeline_text += (
                    f"{i}. {pipeline['status_icon']} {pipeline['name']}{restart_info}\n"
                )
                pipeline_text += f"   â€¢ Table: {pipeline['table']}\n"
                pipeline_text += f"   â€¢ Topic: {pipeline['topic']}\n"

            blocks.append(
                {
                    "type": "section",
                    "text": {"type": "mrkdwn", "text": pipeline_text.strip()},
                }
            )

        # ë¡œê·¸ íŒŒì¼ ì •ë³´ (ê°„ëµí™”)
        log_text = "*Log Files:*\n"
        for log_file in status_data["log_files"]:
            if log_file.get("exists", True):
                log_text += f"â€¢ {log_file['name']}: {log_file['size_kb']} KB\n"
            else:
                log_text += f"â€¢ {log_file['name']}: Not found\n"

        blocks.append(
            {"type": "section", "text": {"type": "mrkdwn", "text": log_text.strip()}}
        )

        return blocks


class PipelineManager:
    """íŒŒì´í”„ë¼ì¸ ê´€ë¦¬ì í´ë˜ìŠ¤"""

    def __init__(
        self, pipelines_config: List[dict], global_stop_event: threading.Event
    ):
        """íŒŒì´í”„ë¼ì¸ ê´€ë¦¬ì ì´ˆê¸°í™”"""
        self.global_stop_event = global_stop_event
        self.pipelines: Dict[str, MySQLKafkaS3Pipeline] = {}
        self.pipeline_threads: Dict[str, Dict[str, threading.Thread]] = {}
        self.pipeline_status: Dict[str, Dict] = {}
        self.restart_counts: Dict[str, int] = {}

        # ìµœëŒ€ ì¬ì‹œì‘ ì‹œë„ íšŸìˆ˜ ì„¤ì • (ì²« ë²ˆì§¸ íŒŒì´í”„ë¼ì¸ ì„¤ì •ì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ ê¸°ë³¸ê°’ ì‚¬ìš©)
        self.max_restart_attempts: int = 10
        if pipelines_config:
            self.max_restart_attempts = pipelines_config[0].get(
                "max_restart_attempts", 10
            )

        # ì‹¤íŒ¨í•œ íŒŒì´í”„ë¼ì¸ ì¶”ì  (ì¬ì‹œì‘ ì œí•œ ë„ë‹¬)
        self.failed_pipelines: set = set()

        # Initialize pipelines
        for pipeline_config in pipelines_config:
            pipeline_name = pipeline_config["name"]
            if pipeline_config.get("enabled", True):
                pipeline = MySQLKafkaS3Pipeline(pipeline_config, global_stop_event)
                self.pipelines[pipeline_name] = pipeline
                self.pipeline_threads[pipeline_name] = {}
                self.pipeline_status[pipeline_name] = {
                    "status": "stopped",
                    "last_update": datetime.now(),
                    "producer_running": False,
                    "consumer_running": False,
                }
                self.restart_counts[pipeline_name] = 0
                print(f"âœ… Pipeline '{pipeline_name}' registered")
            else:
                print(f"â¸ï¸ Pipeline '{pipeline_name}' disabled in config")

        print(f"ğŸ”§ Max restart attempts per pipeline: {self.max_restart_attempts}")

    def start_pipeline(self, pipeline_name: str) -> bool:
        """ê°œë³„ íŒŒì´í”„ë¼ì¸ ì‹œì‘"""
        if pipeline_name not in self.pipelines:
            print(f"âŒ Pipeline '{pipeline_name}' not found")
            return False

        pipeline = self.pipelines[pipeline_name]

        try:
            # Start producer thread
            producer_thread = threading.Thread(
                target=pipeline.mysql_to_kafka_producer_job,
                name=f"{pipeline_name}_producer",
                daemon=True,
            )
            producer_thread.start()
            self.pipeline_threads[pipeline_name]["producer"] = producer_thread

            # Start consumer thread
            consumer_thread = threading.Thread(
                target=pipeline.kafka_to_iceberg_consumer_job,
                name=f"{pipeline_name}_consumer",
                daemon=True,
            )
            consumer_thread.start()
            self.pipeline_threads[pipeline_name]["consumer"] = consumer_thread

            # Update memory status
            self.pipeline_status[pipeline_name].update(
                {
                    "status": "running",
                    "last_update": datetime.now(),
                    "producer_running": True,
                    "consumer_running": True,
                }
            )

            # ğŸ†• íŒŒì´í”„ë¼ì¸ ì‹œì‘ ì‹œ ì¦‰ì‹œ íŒŒì¼ ìƒíƒœë¥¼ successë¡œ ì—…ë°ì´íŠ¸
            self.update_pipeline_file_status(
                pipeline_name, enabled=True, is_failed=False
            )

            print(f"ğŸš€ Pipeline '{pipeline_name}' started successfully")
            return True

        except Exception as e:
            print(f"âŒ Failed to start pipeline '{pipeline_name}': {e}")
            self.pipeline_status[pipeline_name].update(
                {
                    "status": "error",
                    "last_update": datetime.now(),
                    "error": str(e),
                }
            )
            # ğŸ†• ì‹œì‘ ì‹¤íŒ¨ ì‹œ íŒŒì¼ ìƒíƒœë¥¼ failedë¡œ ì—…ë°ì´íŠ¸
            self.update_pipeline_file_status(
                pipeline_name, enabled=False, is_failed=True
            )
            return False

    def stop_pipeline(self, pipeline_name: str) -> bool:
        """ê°œë³„ íŒŒì´í”„ë¼ì¸ ì¤‘ì§€"""
        if pipeline_name not in self.pipelines:
            print(f"âŒ Pipeline '{pipeline_name}' not found")
            return False

        try:
            pipeline = self.pipelines[pipeline_name]
            pipeline.stop_event.set()

            # Wait for threads to stop
            threads = self.pipeline_threads.get(pipeline_name, {})
            for thread_type, thread in threads.items():
                if thread and thread.is_alive():
                    print(
                        f"â³ Waiting for {pipeline_name} {thread_type} thread to stop..."
                    )
                    thread.join(timeout=10)
                    if thread.is_alive():
                        print(
                            f"âš ï¸ {pipeline_name} {thread_type} thread did not stop gracefully"
                        )
                    else:
                        print(f"âœ… {pipeline_name} {thread_type} thread stopped")

            # Clear threads
            self.pipeline_threads[pipeline_name] = {}

            # Reset stop event for potential restart
            pipeline.stop_event.clear()

            # Update memory status
            self.pipeline_status[pipeline_name].update(
                {
                    "status": "stopped",
                    "last_update": datetime.now(),
                    "producer_running": False,
                    "consumer_running": False,
                }
            )

            # ğŸ†• íŒŒì´í”„ë¼ì¸ ì¤‘ì§€ ì‹œ íŒŒì¼ ìƒíƒœë¥¼ stoppedë¡œ ì—…ë°ì´íŠ¸
            self.update_pipeline_file_status(
                pipeline_name, enabled=False, is_failed=False
            )

            print(f"ğŸ›‘ Pipeline '{pipeline_name}' stopped successfully")
            return True

        except Exception as e:
            print(f"âŒ Failed to stop pipeline '{pipeline_name}': {e}")
            return False

    def update_pipeline_file_status(
        self,
        pipeline_name: str,
        enabled: bool,
        is_failed: bool,
        last_processed_id: int = None,
    ):
        """íŒŒì´í”„ë¼ì¸ íŒŒì¼ ìƒíƒœ ì—…ë°ì´íŠ¸ (ì¦‰ì‹œ ë°˜ì˜)"""
        try:
            status_file = PIPELINE_STATUS_FILE

            # ê¸°ì¡´ ìƒíƒœ íŒŒì¼ ë¡œë“œ
            status_data = {}
            if os.path.exists(status_file):
                try:
                    with open(status_file, "r") as f:
                        status_data = json.load(f)
                except:
                    pass

            # í•´ë‹¹ íŒŒì´í”„ë¼ì¸ ì°¾ê¸°
            pipeline_config = None
            for pipeline in PIPELINES:
                if pipeline["name"] == pipeline_name:
                    pipeline_config = pipeline
                    break

            if not pipeline_config:
                print(f"âš ï¸ Pipeline config not found for '{pipeline_name}'")
                return

            # ê¸°ì¡´ ë°ì´í„° ë³´ì¡´
            current_data = status_data.get(pipeline_name, {})

            # ìƒíƒœ ì—…ë°ì´íŠ¸
            status_data[pipeline_name] = {
                "enabled": enabled,
                "is_failed": is_failed,
                "restart_count": current_data.get("restart_count", 0),
                "last_processed_id": (
                    last_processed_id
                    if last_processed_id is not None
                    else current_data.get("last_processed_id", 0)
                ),
                "table_name": pipeline_config["mysql"]["table"],
                "kafka_topic": pipeline_config["kafka"]["topic"],
            }

            # íŒŒì¼ ì €ì¥
            with open(status_file, "w") as f:
                json.dump(status_data, f, indent=2)

            status_text = (
                "SUCCESS"
                if enabled and not is_failed
                else ("FAILED" if is_failed else "STOPPED")
            )
            print(
                f"ğŸ“ Pipeline '{pipeline_name}' file status updated to: {status_text}"
            )

        except Exception as e:
            print(f"âš ï¸ Failed to update pipeline file status: {e}")

    def restart_pipeline(self, pipeline_name: str) -> bool:
        """ê°œë³„ íŒŒì´í”„ë¼ì¸ ì¬ì‹œì‘"""
        # ì¬ì‹œì‘ ì œí•œ í™•ì¸
        current_restart_count = self.restart_counts.get(pipeline_name, 0)
        if current_restart_count >= self.max_restart_attempts:
            error_msg = f"Pipeline '{pipeline_name}' has exceeded maximum restart attempts ({self.max_restart_attempts}). Marking as permanently failed."
            print(f"âŒ {error_msg}")
            self.failed_pipelines.add(pipeline_name)
            return False

        print(
            f"ğŸ”„ Restarting pipeline '{pipeline_name}'... (attempt {current_restart_count + 1}/{self.max_restart_attempts})"
        )

        if self.stop_pipeline(pipeline_name):
            time.sleep(2)  # Brief pause between stop and start
            if self.start_pipeline(pipeline_name):
                self.restart_counts[pipeline_name] = current_restart_count + 1
                print(
                    f"âœ… Pipeline '{pipeline_name}' restarted successfully (restart count: {self.restart_counts[pipeline_name]})"
                )
                return True

        print(f"âŒ Failed to restart pipeline '{pipeline_name}'")
        return False

    def start_all_pipelines(self) -> None:
        """ëª¨ë“  íŒŒì´í”„ë¼ì¸ ì‹œì‘"""
        print("ğŸš€ Starting all pipelines...")

        # ğŸ†• íŒŒì´í”„ë¼ì¸ ì‹œì‘ ì‹œ ì‹¤íŒ¨ ìƒíƒœ ë° ì¬ì‹œì‘ ì¹´ìš´íŠ¸ ì´ˆê¸°í™”
        print("ğŸ”„ Resetting pipeline failure states and restart counts...")
        self.failed_pipelines.clear()
        self.restart_counts.clear()
        for pipeline_name in self.pipelines.keys():
            self.restart_counts[pipeline_name] = 0

        for pipeline_name in self.pipelines.keys():
            self.start_pipeline(pipeline_name)

        print("âœ… All pipelines start sequence completed")

    def stop_all_pipelines(self) -> None:
        """ëª¨ë“  íŒŒì´í”„ë¼ì¸ ì¤‘ì§€"""
        print("ğŸ›‘ Stopping all pipelines...")

        for pipeline_name in self.pipelines.keys():
            self.stop_pipeline(pipeline_name)

        print("âœ… All pipelines stopped")

    def get_pipeline_status(self, pipeline_name: str) -> Dict:
        """ê°œë³„ íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì¡°íšŒ"""
        if pipeline_name not in self.pipelines:
            return {"error": f"Pipeline '{pipeline_name}' not found"}

        pipeline = self.pipelines[pipeline_name]
        threads = self.pipeline_threads.get(pipeline_name, {})

        # Check thread status
        producer_alive = threads.get("producer") and threads["producer"].is_alive()
        consumer_alive = threads.get("consumer") and threads["consumer"].is_alive()

        # Get circuit breaker status
        cb_status = pipeline.get_circuit_breaker_status()

        status = {
            "name": pipeline_name,
            "status": "running" if (producer_alive and consumer_alive) else "stopped",
            "producer_running": producer_alive,
            "consumer_running": consumer_alive,
            "last_processed_id": pipeline.last_processed_id,
            "restart_count": self.restart_counts.get(pipeline_name, 0),
            "circuit_breaker": cb_status,
            "last_update": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        }

        # ğŸ†• ì²˜ë¦¬ ì§€ì—° ì •ë³´ ì¶”ê°€
        try:
            lag_info = pipeline.get_processing_lag_info()
            status["lag_info"] = lag_info
        except Exception as e:
            print(f"âš ï¸ Failed to get lag info for {pipeline_name}: {e}")
            status["lag_info"] = {"error": str(e)}

        # Update internal status
        self.pipeline_status[pipeline_name] = status

        return status

    def get_all_pipeline_status(self) -> Dict[str, Dict]:
        """ëª¨ë“  íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì¡°íšŒ"""
        all_status = {}
        for pipeline_name in self.pipelines.keys():
            all_status[pipeline_name] = self.get_pipeline_status(pipeline_name)
        return all_status

    def print_pipeline_status(self) -> None:
        """íŒŒì´í”„ë¼ì¸ ìƒíƒœë¥¼ ì½˜ì†”ì— ì¶œë ¥"""
        print("\n" + "=" * 80)
        print("ğŸ“Š PIPELINE STATUS REPORT")
        print("=" * 80)

        all_status = self.get_all_pipeline_status()

        if not all_status:
            print("âŒ No pipelines configured")
            return

        for pipeline_name, status in all_status.items():
            print(f"\nğŸ”§ Pipeline: {pipeline_name}")

            # ì‹¤íŒ¨í•œ íŒŒì´í”„ë¼ì¸ í‘œì‹œ
            if pipeline_name in self.failed_pipelines:
                print(f"   Status: ğŸ’€ PERMANENTLY FAILED (Max restarts exceeded)")
            else:
                print(
                    f"   Status: {'ğŸŸ¢ RUNNING' if status['status'] == 'running' else 'ğŸ”´ STOPPED'}"
                )

            print(f"   Producer: {'âœ…' if status['producer_running'] else 'âŒ'}")
            print(f"   Consumer: {'âœ…' if status['consumer_running'] else 'âŒ'}")
            print(f"   Last Processed ID: {status['last_processed_id']}")
            print(
                f"   Restart Count: {status['restart_count']}/{self.max_restart_attempts}"
            )

            # ğŸ†• ì²˜ë¦¬ ì§€ì—° ì •ë³´ í‘œì‹œ
            if "lag_info" in status and "error" not in status["lag_info"]:
                lag_info = status["lag_info"]
                print(f"   ğŸ“Š Processing Lag Info:")
                print(f"      MySQL Max ID: {lag_info['mysql_max_id']}")
                print(f"      Last Processed ID: {lag_info['last_processed_id']}")
                print(f"      Total Lag: {lag_info['total_lag']} records")

                # ì§€ì—° ìƒíƒœì— ë”°ë¥¸ ê²½ê³  í‘œì‹œ
                if lag_info["total_lag"] > 10000:
                    print(f"      âš ï¸ High lag detected!")

            # Circuit breaker status with enhanced display
            cb_status = status["circuit_breaker"]
            cb_state = cb_status["state"]

            if cb_state == CircuitBreakerState.CLOSED:
                cb_emoji = "ğŸŸ¢"
                cb_text = "CLOSED"
            elif cb_state == CircuitBreakerState.OPEN:
                cb_emoji = "ğŸ”´"
                cb_text = "OPEN"
            else:  # HALF_OPEN
                cb_emoji = "ğŸŸ¡"
                cb_text = "HALF_OPEN"

            print(f"   Circuit Breaker: {cb_emoji} {cb_text}")
            print(f"   Consecutive Failures: {cb_status['consecutive_failures']}")

            if cb_state == CircuitBreakerState.OPEN:
                print(f"   Recovery Time: {cb_status['time_until_recovery']:.1f}s")

            print(f"   Last Update: {status['last_update']}")

        print("\n" + "=" * 80)
        print("ğŸ“‹ CONFIGURATION")
        print("=" * 80)

        # Show configuration for first pipeline as example
        if self.pipelines:
            first_pipeline = next(iter(self.pipelines.values()))
            print(
                f"ğŸ”§ Stop Producer on Iceberg Failure: {first_pipeline.stop_producer_on_iceberg_failure}"
            )
            print(
                f"ğŸ“Š Max Kafka Lag Tolerance: {first_pipeline.max_kafka_lag_tolerance}"
            )
            print(f"ğŸ”„ Max Restart Attempts: {self.max_restart_attempts}")

        print("=" * 80)

    def monitor_pipelines(self) -> None:
        """íŒŒì´í”„ë¼ì¸ ëª¨ë‹ˆí„°ë§ (ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰)"""
        print("ğŸ‘ï¸ Starting pipeline monitoring...")

        while not self.global_stop_event.is_set():
            try:
                # ëª¨ë“  íŒŒì´í”„ë¼ì¸ì´ permanently failed ìƒíƒœì¸ì§€ í™•ì¸
                if (
                    len(self.failed_pipelines) == len(self.pipelines)
                    and len(self.pipelines) > 0
                ):
                    print(
                        f"ğŸ’€ All pipelines have permanently failed! Shutting down entire process..."
                    )
                    print(f"   Failed pipelines: {', '.join(self.failed_pipelines)}")
                    print(
                        "   Use 'python pipeline_manager.py restart' to reset and try again"
                    )
                    # ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œë¥¼ ìœ„í•´ global_stop_event ì„¤ì •
                    self.global_stop_event.set()
                    break

                # Check each pipeline
                for pipeline_name in self.pipelines.keys():
                    if self.global_stop_event.is_set():
                        break

                    # ì‹¤íŒ¨í•œ íŒŒì´í”„ë¼ì¸ì€ ì¬ì‹œì‘ ì‹œë„í•˜ì§€ ì•ŠìŒ
                    if pipeline_name in self.failed_pipelines:
                        continue

                    status = self.get_pipeline_status(pipeline_name)

                    # Check if pipeline should be running but isn't
                    if status["status"] == "stopped":
                        print(
                            f"âš ï¸ Pipeline '{pipeline_name}' is stopped - attempting restart"
                        )
                        if not self.restart_pipeline(pipeline_name):
                            print(
                                f"âŒ Failed to restart pipeline '{pipeline_name}' - monitoring disabled for this pipeline"
                            )

                # ì‹¤íŒ¨í•œ íŒŒì´í”„ë¼ì¸ì´ ìˆì§€ë§Œ ëª¨ë“  íŒŒì´í”„ë¼ì¸ì´ ì‹¤íŒ¨í•œ ê²ƒì€ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ì•Œë¦¼
                if self.failed_pipelines and len(self.failed_pipelines) < len(
                    self.pipelines
                ):
                    print(
                        f"âš ï¸ Failed pipelines (restart limit reached): {', '.join(self.failed_pipelines)}"
                    )

                # Wait before next check
                self.global_stop_event.wait(30)  # Check every 30 seconds

            except Exception as e:
                print(f"âŒ Error in pipeline monitoring: {e}")
                self.global_stop_event.wait(10)  # Wait before retrying

        print("ğŸ‘ï¸ Pipeline monitoring stopped")

    def get_pipeline_names(self) -> List[str]:
        """ë“±ë¡ëœ íŒŒì´í”„ë¼ì¸ ì´ë¦„ ëª©ë¡ ë°˜í™˜"""
        return list(self.pipelines.keys())


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description="MySQL â†’ Kafka â†’ S3 íŒŒì´í”„ë¼ì¸ ê´€ë¦¬ ë„êµ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  %(prog)s start                          # íŒŒì´í”„ë¼ì¸ ì‹œì‘
  %(prog)s stop                           # íŒŒì´í”„ë¼ì¸ ì¤‘ì§€  
  %(prog)s restart                        # íŒŒì´í”„ë¼ì¸ ì¬ì‹œì‘
  %(prog)s restart-single <pipeline_name> # ê°œë³„ íŒŒì´í”„ë¼ì¸ ì¬ì‹œì‘
  %(prog)s stop-single <pipeline_name>    # ê°œë³„ íŒŒì´í”„ë¼ì¸ ì¤‘ì§€
  %(prog)s status                         # íŒŒì´í”„ë¼ì¸ ìƒíƒœ í™•ì¸
  %(prog)s logs                           # ìµœê·¼ ë¡œê·¸ ë³´ê¸°
  %(prog)s logs --lines 100               # ìµœê·¼ 100ì¤„ ë¡œê·¸ ë³´ê¸°
        """,
    )

    parser.add_argument(
        "action",
        choices=[
            "start",
            "stop",
            "restart",
            "restart-single",
            "stop-single",
            "status",
            "logs",
        ],
        help="ìˆ˜í–‰í•  ì‘ì—…",
    )

    parser.add_argument(
        "pipeline_name",
        nargs="?",
        help="ê°œë³„ íŒŒì´í”„ë¼ì¸ ì‘ì—…ì‹œ íŒŒì´í”„ë¼ì¸ ì´ë¦„ (restart-single, stop-singleìš©)",
    )

    parser.add_argument(
        "--lines",
        "-n",
        type=int,
        default=50,
        help="ë¡œê·¸ ë³´ê¸°ì‹œ í‘œì‹œí•  ì¤„ ìˆ˜ (ê¸°ë³¸ê°’: 50)",
    )

    args = parser.parse_args()

    manager = PipelineManagerCLI()

    # ì•¡ì…˜ ì‹¤í–‰
    if args.action == "start":
        success = manager.start_pipeline()
        sys.exit(0 if success else 1)

    elif args.action == "stop":
        success = manager.stop_pipeline()
        sys.exit(0 if success else 1)

    elif args.action == "restart":
        success = manager.restart_pipeline()
        sys.exit(0 if success else 1)

    elif args.action == "restart-single":
        if not args.pipeline_name:
            print("âŒ Error: Pipeline name is required for restart-single")
            print("Usage: python pipeline_manager.py restart-single <pipeline_name>")
            sys.exit(1)
        success = manager.restart_single_pipeline(args.pipeline_name)
        sys.exit(0 if success else 1)

    elif args.action == "stop-single":
        if not args.pipeline_name:
            print("âŒ Error: Pipeline name is required for stop-single")
            print("Usage: python pipeline_manager.py stop-single <pipeline_name>")
            sys.exit(1)
        success = manager.stop_single_pipeline(args.pipeline_name)
        sys.exit(0 if success else 1)

    elif args.action == "status":
        manager.status_pipeline()
        sys.exit(0)

    elif args.action == "logs":
        manager.show_logs(args.lines)
        sys.exit(0)


if __name__ == "__main__":
    main()
